DISTRIBUTED REPRESENTATIONS, PART-WHOLE RELATIONSHIPS AND ROUTING

THE DOT-PRODUCT SIMILARITY MEASURE FOR REPRESENTING A PATTERN OF EMBEDDING
VECTORS.

I was thinking about how convolutional filters work together to slowly
build up the recognition of complex objects.  The particular interpretation
of the input and output channels that I use is the following:

The filter consists of a local spatial patch of units, each with a
position-specific matrix.  We can think of the filter decomposed by output
channels.  Each output channel recognizes a separate archtypal pattern of
vectors.  Each unit in the patch carries with it a specific vector of weights
that represents the archtypal vector.  The input contains vectors of
input-channel dimension, and as the filter passes over the input, the weights
vector in each filter cell is dot-product'ed with the input.  The resulting
scalar value is a measure of:

1. the length of the input vector
2. how closely the direction of the input vector matches the weights vector

Note that the length of the weights vector is not important in discerning
patterns, since it remains constant over the course of inference.

One might think of individual entities as inhabiting surface regions on the
unit hypersphere, and perhaps the probability of their presence as the norm of
the embedding.

Once all dot products are computed, they are summed, to give a scalar value
representing the "strength of presence" of the particular pattern.  Thinking of
many of these patterns as "basis patterns", one can imagine being able to
detect a large number of patterns by expressing them as combinations of basis
patterns.

SUMMARY

So, to summarize, a convolutional filter is a set of N (# output channels)
patterns, each one being a spatial grid of ideal subpatterns.  Each subpattern
is itself expressed as a distributed representation.  And, the N patterns form
a sort of set of "basis patterns", which when combined, express a large set of
patterns in distributed fashion.

Combining a single pattern with input gives a scalar value indicating how
strongly the input resembles the archtypal pattern that is expressed in the
weights. 

A CONVOLUTIONAL FILTER DEFINES PATTERNS WITH FIXED SCALE AND FIXED SPATIAL
ARRANGEMENT OF SUBPATTERNS

It is important to realize the limitations of a convolutional filter.  First,
it is not scale invariant.  If an image is scaled, the embedded representation
of a filter changes completely, and there is no relation between the embedding.

Secondly, a pattern, which recognizes particular subpatterns at each position
in the grid, cannot recognize any variation in the spatial arrangement of these
sub-patterns.


THE ROLE OF POOLING

So, a slightly misplaced eyeball (or wheel, or other sub-entity within an
entity) will thus fail to activate the appropriate sub-pattern at the expected
location in the filter, and thus it will be as if it isn't there at all.   The
idea of pooling is a filter that can be seen as a family of coordinated filters
that are in winner-take-all competition with each other.  Each filter
represents a specific pattern, which, typical of ordinary filters, means that
each subpattern must occur at an exact location within the patch.  But, now, we
have a combinatorial family of filters, constructed by considering the
combination of P distinct sub-positions of each subpattern.  Thus, logically,
there are P^G distinct filters (G = # of grid elements), and the score given by
the best matching one will be propagated forward.

But...I just did this experiment and it looks like max pooling is done
channel-wise, which destroys any information about embedding relationships.  In
the code below, there are three channels, and a field of size 4 over which the
pooling is happening.  You can see that the first channel receives its value
from the third cell, the second channel from the first cell, and the third
channel from the second cell.  


>>> m = nn.MaxPool1d(4, stride=1)
>>> input = torch.randn(1, 3, 4)
>>> input
tensor([[[-0.0727, -0.9561,  1.0411,  0.0589],
         [ 1.4134,  1.3270, -0.5751, -0.2381],
         [-0.1564,  0.3408, -1.5731, -1.2075]]])
>>> output = m(input)
>>> output
tensor([[[1.0411],
         [1.4134],
         [0.3408]]])

What I had thought would make a lot more sense would be the notion that the
convolutional filter is "looking" for the single best-fitting feature in a
limited region within its overall receptive field.  Best-fitting means it takes
O (# output channels) dot products with each of the competing input cells.  It
has then computed an embedding vector.

ROUTING CONFIGURATION

Max pooling can be seen as a form of routing, if we adopt the following
definition of routing:

During inference, incoming neuronal weights are fixed, and so, a neuron's input
at a particular moment varies only with the activations of its source neurons.
This is fixed routing, because the information of their activations is flowing
in a fixed pattern.  With dynamic routing, there is an activity-dependent
weighting which censors certain communication lines.

With Hinton's capsules, routing is accomplished through b_ij values, which are
iteratively updated.  In max pooling, a single O(N) search for the maximum
activation value results in setting all but one weight to zero.

So, let's define the notion of a "routing coefficient" as the weight applied to
a particular neuron-to-neuron connection, calculated dynamically from activations
at a particular moment.  Then, a routing configuration consists of the values of
all routing coefficients.

Now, the question is, can we design a convolutional filter mechanism that
efficiently computes the logically "best" encoding, among the P^G different
routing configurations?

If there were only one output channel, then the max-conv output would be a
single scalar, and we would simply search for the configuration with the
largest scalar value.  Since this scalar is just a sum of dot products, the
routing configuration is decomposable and would just be sum(max_p(cell_j)).
But, if we wanted to maximize the norm of the encoding vector (supposing that
we interpret the encoding norm as the probability of presence of this feature)
then, we cannot decompose it like this.















