NOTES ON WAVENET AUTOENCODER PAPER (Chorowski, January 2019)

Conceptual difficulty: an autoencoder produces a bottleneck that forces the
representation to throw out "noise" while retaining the ability to reconstruct
the input.

Meanwhile, the goal of the paper is to learn a representation that factors out
voice qualities like timbre, pitch, tempo from phoneme content.  There doesn't
seem to be any constraint in the training regime that would encourage learning
such a representation.

The VQ-VAE uses a deterministic encoder procedure.  It seems like the first
part of the encoding procedure is the same as in a VAE: a NN is used to compute
parameters for a multivariate gaussian, and a sample is taken from that.  But,
then, a quantization procedure selects the nearest (L2) "prototype vector" from
a "maintained" set of K prototype vectors, and that is used as the latent
representation.  To compute the gradient, the VQ-VAE uses something called a
'straight-through estimator' for computing the gradient.  Also, the prototype
vectors are trained using a "commitment loss".


Importantly, they argue that if they assume a "uniform prior" over the K
prototype vectors, then the KL-divergence is always equal to log K.  Instead it
is a hyperparameter "tied to the size of the prototype inventory"  This seems a
bit erroneous of an interpretation, because the notion of the KL divergence
seems to me important to assume a *fixed* prior p(z).  In this case, the K
prototype vectors are being trained.  However, I note that the commitment loss
or other loss seems to take the place of KL divergence here as regularizer /
bottleneck.

(Must read [18] Neural Discrete Representation Learning)

"A downside of autoregressive models is that they do not explicitly produce
latent representations of the data"

Why is this, and what does this statement mean?  Well, going back to basics,
the autoregressive model learns a conditional probability distribution that is
useful for computing p(x) (x is a sequence) and generating samples x ~ p(x)
efficiently.  I don't really have experience with autoregressive models besides
WaveNet and PixelCNN / PixelRNN.  But, with both of those, it seems that they
need very rich local conditioning vectors to guide their production of anything
useful.  And, the local conditioning information must be ultimately provided at
the resolution of the input/output domain (each time step of a waveform, or
each pixel of an image).


Section II C: Latent space collapse: A phenomenon in which the decoder learns
to ignore the representation and only uses the "unconstrained signal coming
through the autoregressive path".

This is confusing because, in my experience with WaveNet, it is incapable of
producing anything but static without local conditioning vectors.  However, I
do note that in that case, WaveNet is trained just to maximize probability of
the data.  This objective is quite different than a MSE loss between the
generated waveform and the input waveform.  But, frankly, I don't see how a
simple autoregressive model could be made to achieve that.

Section III: "Explicitly conditioning the decoder on speaker identity frees the
encoder from having to capture speaker-dependent information in the latent
representation".

Yes, this is true.  But, does it actively drive the encoder to produce a
speaker-independent representation?  I can think of a justification for that,
but not sure...

Page 4 first PP: "...and latent information about *past and future* samples extracted
by the encoder"

This is confusing.  They say they are using traditional WaveNet as the decoder,
but the decoder doesn't use future samples (unless they mean the small window
of future samples that will be used in the upsampling process to generate
per-time-step conditioning vectors)

IDEA FOR VARIABLE-DURATION FEATURE EXTRACTION

The LSTM-based sequence transformer has an output gate that can decide whether
or not to output something at a given timestep.  Is it possible that one could
be trained to learn *when* to output something, based on seeing a complete
pattern of, say, a phoneme in a waveform?

See p. 5 top for WaveNet architecture.

Section IV E 2: Input representation:  We then replaced the waveform with a
customary ASR pipeline: 80 log-mel filterbank features extracted every 10 ms
from 25ms-long windows and 13 MFCC features extracted from the mel-filterbank
output, both augmented with their first and second temporal derivatives.

From https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html




MODEL CONSTRUCTION

Encoder (Section IV A)

Waveform (16 steps / ms)
MFCC + d + a (every 160 steps or 10 ms) (see IV: E 2: Input representation)
Conv(3, 1, 39, 768) + Residual  (3 frame rf)
Conv(3, 1, 768, 768) + Residual  (5 frame rf)
Conv(4, 2, 768, 768)             (8 frame rf)
Conv(3, 1, 768, 768) + Residual  (12 frame rf)
Conv(3, 1, 768, 768) + Residual  (16 frame rf) (confirmed in text)
FC(ReLU, 768, 768) + Residual
FC(ReLU, 768, 768) + Residual
FC(ReLU, 768, 768) + Residual
FC(ReLU, 768, 768) + Residual

"The resulting latent vectors were extracted at 50 Hz (i.e. every second frame),
with each latent vector depending on a receptive field of 16 input frames."

Here, a "frame" is a 39D derivative-augmented MFCC coefficient set.  

I believe that what they call a 'feedforward' layer is just a size-one convolution.


OVERALL TRAINING PROCEDURE

How is the autoencoder trained?  The encoder receives a MFCCda vector,
converting it into a sequence of feature vectors at a fixed down-sampling ratio
(1/2).  Each element in this sequence is passed through one of the bottlenecks,
which outputs a same-length sequence of smaller feature vectors.  These are
then jittered, convoluted, and upsampled to the time-domain frequency using a
deconvolution.  The one-hot speaker ID vector is concatenated to every time
step.

At this point, classical WaveNet is run.  I believe it must be run in teacher
forcing mode, in which the correct amplitude is fed at every timestep, and the
loss is calculated as the cross-entropy between the outputted softmax and the
quantized one-hot amplitude at the next timestep.

This accounts for the WaveNet parameters, but how does the Encoder get trained?
From the perspective of WaveNet, training must be even more complex than with
fixed data, because, each time WaveNet sees the same waveform, it will be
receiving a different set of local conditioning vectors.

From the perspective of the Encoder, all we really need to train it is a gradient
of the loss with respect to each component of the embedding vectors, and all
of that is available.

The logic of batching seems to follow the same pattern as with WaveNet.
Namely, the individual training datum is a single waveform and a single
timestep.  Batching across samples, and batching for a window of timesteps
are both valid.

The issue with autoregressive state is still present.  It is a consequence of
the fact that the prediction for each timestep depends on a window of N
previous timesteps.  This means that at each layer in WaveNet, any two
consecutive timestep predictions will share R-1 convolution results.  The
earlier timestep lacks exactly one convolutional result (the newest), and the
later timestep will not use the oldest convolutional result of the earlier
timestep.  Besides these, both timesteps share R-1 convolutional results at
each layer.  These should be re-used.


During training, we are using teacher forcing, which breaks the dependence
between timesteps.  So, we can batch a window of timesteps as well as samples.
This then has a few parts to it.  First is the exponential sawtooth shaped set
of left-ward back buffers, defined as the receptive fields of a single
convolution at time 0 for each layer.  Having these values populated at each
layer allows unpadded convolutions to be carried out layer by layer, and
correctly resume the calculation.

Another thing is that the bottleneck only needs to be run during training, and
so we can write it to batch a set of samples across time steps.

But, we can just use a reparameterization trick to achieve a set of random samples
from different 




Bottlenecks

VQ-VAE
Linear(768, 64)
VQ  (select nearest prototype vector)

VAE
Linear(768, 128)
Mu + Sigma
Sample

AE
Linear(768, 64)


Decoder

GatedResidualConv(







