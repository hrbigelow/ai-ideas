Supervised training using associative memory


Note that the quality or utility of an encoding function is not a well defined
concept.  It becomes well defined only when a decoding function is also
defined. Thus the chain: input -> code -> output is defined, and the input to
output mapping can be then evaluated for its quality against a gold standard of
input -> output pairings.  In a simple MLP, this is achieved through SGD on
labeled data.

For reasons explained below, I adopt a slightly more elaborate scheme of input
-> code1 -> code2 -> output, with code1 -> code2 link being implemented by an
associative memory.  An MLP with more than three layers of nodes can be
interpreted this way.  I assume also that the memory works in such a way that
querying with code1' (nearby to code1) will retrieve a code2' (nearby code2).

But, let's suppose that we have a conventional hash table that stores the
code1/code2 associations as key/value pairs.  In other words, the code1 ->
code2 part of the chain has no ability to generalize beyond the training data.
So, in order for the entire chain to produce any output on a new input, the
encoder would need to learn to intercalate input space, *and* to produce
exactly one of the existing N code1's from the training data.

For an encoder to map entire volumes of a high-dimensional input space to the
same code would likely be bad, because it would mean that it was not faithfully
carrying equivariant properties through into the code.  So, I'll assume a
"good" encoder will not do this.

What about the decoder?  Typically, the output space will be small, at least in
this setting where we are not considering recurrence, only mapping individual
patterns to output patterns.  Examples would include labeling and things
involving actions.  So, unlike the encoder, a good decoder might very well
map volumes of code2 space to single outputs.

At the moment, I don't know exactly what effect having an approximate-match
memory might have on an MLP.

TRAINING IDEAS

Ordinarily, performing supervised training of an MLP requires repeated exposure
to each training example, and it could be done in minibatch SGD.  In this
setup, at each step, the individual errors from each example in the minibatch
are averaged, and weight updates applied throughout the layers.

What about the Kanerva MLP?  In this case, we want to simultaneously train
the encoder, decoder, and store code1 -> code2 associations.  The problem
is that there is no way to generate code2 in this regime.  We need a separate
encoder for the target.

But, note that this is not inconsistent with what a human would do.  Consider:
if a human were faced with the task of captioning pictures, and he were given
captioned pictures as a training set, and uncaptioned pictures as a test set,
the process of learning would involve:

1. learn how to encode pictures (to see and interpret)
2. learn how to encode captions (to read words)
3. having both encodings, create associations from one to the other.
4. learn how to decode caption codes (to speak)

Then, during inference time:

1. interpret the image
2. retrieve an associated word encoding
3. speak the encoding



In any case, in training a feed-forward model using back-propagation, the
paradoxical thing is that the target caption is not actually interpreted.
Instead, the caption produced by the speaking model is compared with the target
caption, and an error signal is produced.

This is actually quite suboptimal.  The fact that the caption is of a modest
dimension somewhat disguises the problem.  If we consider the inverse activity,
drawing a picture from a caption, the problem becomes crystal clear.

In that problem, we provide input captions "labeled" with target pictures.
During test time, we are just given captions and asked to generate pictures.
It is immediately clear that it would be impossible to train such a model,
because the high dimension of the pictures makes it impossible to define a
sensible loss function.  But, why?  Besides the high dimensionality, what
is different about this problem than the reverse problem?

LOW DIMENSIONAL OBSERVABLES DISGUISE OVERFITTING

A simple loss function, such as edit distance, can give a reasonable
measure of difference between, say, the "correct" caption and the inferred
one.  It seems somewhat reasonable to apply a loss function in this way
to train a captioning program.

But, if we think about the reverse problem, drawing a picture to go with a
caption, it seems almost non-sensical to try to apply a loss function to
measure the difference between the "correct" picture and the generated picture.
The pictures are such high dimensional objects that it is hard to see how one
could get a meaningful distance measure.  The only real distance measure that
makes sense is not one done in pixel space, but in a semantic code space.

But, this now reveals the more subtle problem in the original captioning
program: the "reasonable" loss function that measures edit distance between
captions isn't reasonable at all - it is completely inappropriate to measure
edit distance between sentences.  After all, we are interested in *meaning*,
and that can only be measured in code space, not in the observable space.

The situation is exactly the same as with the drawing problem.

If we do train the captioning program in this way, we are overfitting the model
to produce overly specific captions.  Each picture is arbitrarily given one
specifically worded caption, when a few others would have done just fine.  What
we really want the model to learn isn't that one specific caption for each
specific picture - but a family of captions that all mean the same thing.  And,
essentially, the way to achieve this is to train on code space.

The only way to achieve this training regime, though, is to simultaneously
train an encoder and a decoder for the output domain.

SEMANTICS OF THE TRANSFORMATION PROBLEM LOSS CONCEPT

If we are in a situation in which multiple labels are all valid for a given
input datum, then it is an abuse of the principle of supervised learning to use
only a subset of those labels for supervision.  One could suggest augmenting
the training set by adding in those labels and training on the augmented set.
But, the problem gets impractical when the labels are from a very
high-dimensional space, because it is impossible to enumerate all valid labels.

A sensible alternative is to co-train an encoder and decoder for the label
domain.  Then, the decoder generates a potential output, and in order to
compare it with the provided output label, we use the encoder to re-encode
both, and compare them using euclidean distance.  





TRAINING ALGORITHM

The model consists of an image encoder and language encoder, a Kanerva-type
memory, a language decoder.  The two codes are fed into the Kanerva memory
during training to create an association.  Later, image code query retrieves
the associated language code.

The problem contains a subproblem within it, which is how to train the image
model (encoder and decoder).  There are a few issues and ideas.






 






MULTIPLE ENCODERS AND DECODERS

One might object with a counterexample:  suppose I have an encoding process for
facial recognition and I notice that one or more components of my encoding
process seem to track facial identity (i.e. identifies different faces) while
another smoothly varies with head attitude.  I have seemingly evaluated my
encoding process without a decoder.  Or have I?  No, implicitly in this problem
statement, I presented raw images which were faces, but in even speaking of
"facial identity" or "head attitude", I've implicitly assigned a matching
utput pattern to the input.  And, the simple act of inspecting the encoding
and looking for individual elements that seem to track these things *is* a form
of (simple) decoding.  It could well have been a more complex relationship
between the encoding vector and my implicit labels. 

But, where is the code2 in this scenario?  We seem to have instead defined the
chain: input -> code1 -> output.   Is a code2 necessary?  Consider the
situation in which there are say, three input types and three output types, and
we would like to learn nine separate models, one for each combination of input
type and output type.

We could naively do supervised training on each of the nine pairings.
Intuitively this seems wasteful on both ends, however.  It seems that a better
approach would be to learn one encoder, one decoder, and record the pairwise
associations between input and output in an associative memory.

At first glance, it would work like this:

1. Specify one of the three input data types, choose appropriate encoder
2. Specify one of the three output data types, choose appropriate decoder
3. The encoder calculates code1 from input
2. code1 queries memory, retrieving code2
3. The decoder decodes code2 into the output format

But, there is a problem.  Once we have encoded the input, there is no way to
inform the memory which type of association we want to retrieve, but the
associative memory is only capable of storing one association per vector.
What to do?  The simplest 


 

In an artificial model, how might one use this in a real situation?  Here is a proposal:

1. Assume *some* level of pretraining, which we assume comes to us through evolution.

2. Assume an encoder/decoder architecture, although the decoder need not decode to the
same form as the encoder.

Loop:

A. Present the input part of a datum to the encoder to generate an encoding.
B. Feed the encoding to the decoder, generating a prospective output.
C. 
